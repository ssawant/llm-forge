{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaii7VtWgEXULlnMaXE/8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssawant/llm-forge/blob/main/foundational_models/pytorch_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "`model.py`"
      ],
      "metadata": {
        "id": "0X8KFcMFh8Cy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_2tsNNGIg17o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Embedding"
      ],
      "metadata": {
        "id": "A3qCsjQZyo_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "XpK5G3kIelKQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding"
      ],
      "metadata": {
        "id": "7Qded-oOzNuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Create a matrix os shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len, 1)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply the sin to even position\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply the cos to odd position\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "\n",
        "        # Now save the position encoding\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.size[1], :]).requires_grad_(False) # pos enco will not change during training\n",
        "        return self.dropout(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "5a5M2GUde3Yz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "GqbVhR8Y727_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer normalization"
      ],
      "metadata": {
        "id": "Qqg37PTu75JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "      def __init__(self, eps: float = 10**6) -> None:\n",
        "          super().__init__()\n",
        "          self.eps = eps\n",
        "          self.alpha = nn.Parameter(torch.ones(1)) # Multiplied\n",
        "          self.bias = nn.Parameter(torch.zeros(1)) # Added\n",
        "\n",
        "      def forward(self, x):\n",
        "          mean = x.mean(dim=-1, keepdim=True)\n",
        "          std = x.std(dim=-1, keepdim=True)\n",
        "          return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "hcKmnzeq0LJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed Forward"
      ],
      "metadata": {
        "id": "3PlUZQuHCK09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 and B1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 and B2\n",
        "\n",
        "    def forward(self, x):\n",
        "      # (Batch, Seq_Len, d_model) --> (Batch, seq_len, d_ff) --> (Batch, Seq_Len, d_model)\n",
        "      return self.linear_2(self.dropout(self.linear_1(x)))"
      ],
      "metadata": {
        "id": "aVKr0goqCNHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}